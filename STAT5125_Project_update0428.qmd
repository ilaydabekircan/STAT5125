---
title: "US City Population Prediction Project"
format: html
author: "Hei Yee Lau & Ilayda Bekircan"
editor: visual
editor_options: 
  chunk_output_type: inline
---

## OBJECTIVE

The objective of this project is to conduct a comprehensive analysis of demographic information of cities in the United States. Through statistical processing and predictive modeling techniques, we aim to uncover patterns and trends in population dynamics, including factors such as age distribution, ethnic variety and house prices. By understanding the relationship between these variables, this project seeks to develop accurate predictive models capable of forecasting the population sizes for the selected cities. Additionally, the study intends to provide valuable insights into the social, economic, and environmental factors influencing population.

This project involves data cleaning, integration of several data sets, and imputation to ensure data quality. Following the preprocessing, we conduct exploratory analysis to gain insights into the demographic characteristics of various U.S. cities. Then, the data will be divided into training and testing sets for the evaluation of predictive models. Several regression-based models, including linear regression, linear regression with principal component analysis (PCA), and linear regression with penalty, along with non-linear models such as k-nearest neighbors (KKNN) and random forest, will be implemented and compared by using key performance metrics including Root Mean Squared Error (RMSE), R-squared (R\^2), and Mean Absolute Error (MAE). By evaluating different modeling approaches and conducting PCA analysis, this project seeks to provide valuable insights about the predictive capabilities of various regression techniques and their applicability to demographic data analysis.

## DATA DESCRIPTION

There are four different data sets used in this project to gather information about the population, demographic information and some external information that might affect population of cities in the United States. Below are the data sets along with explanations of their features. Any removed features are not explained in this section.

The first data set is sourced from Wikipedia (<https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population>) to have the population information which is the target of the predictive models in this project.

**city:** the name of the city in the United States

**state_code:** the code assigned to the state in which the city is located and it is typically standardized with two-letter abbreviations

**population:** the number of individuals residing within the city limits and this is the target variable of the models in this project

**land_area_km2:** the total land area occupied by the city, measured in square kilometers

The second data set contains information mainly about city demographics which is sourced from <https://public.opendatasoft.com/explore/dataset/us-cities-demographics/table/?flg=en-us> . It also has city and state code features which will be used to join the data after cleaning all individual data sets.

**state:** the full name of the state in which the city is located

**median_age:** the median age of the population within the city which represents the midpoint of the age distribution

**male:** the percentage of the male residents within the city

**female:** the percentage of the female residents within the city

**average_household_size:** the average number of individuals living in each household within the city which can influence various social and economic factors

**white:** the percentage of individuals who identify themselves as White or Caucasian within the city

**american:** the percentage of individuals who identify themselves as American Indian or Alaska Native within the city

**hispanic:** the percentage of individuals who identify themselves as Hispanic or Latino within the city

**black:** the percentage of individuals who identify themselves as Black or African American within the city

**asian:** the percentage of individuals who identify themselves as Asian within the city

**perc_veterans:** the percentage of residents within the city who are military veterans

**perc_foreign_born:** the percentage of residents within the city who were born outside of the United States which indicates the level of international migration and cultural diversity within the population

The third data set is obtained from Kaggle (<https://www.kaggle.com/datasets/denissad/us-cities>) It provides insights about various city attributes, including economic indicators, demographic characteristics, and quality of life metrics.

**region:** thegeographical area in which the city belongs and the regions are given as factors with Northeast, Midwest, South and West

**size:** thesize of the city given as factors with large, mid-sized, small

**avg_rent:** the average rental price for housing which provides information about cost of living and housing affordability for residents

**median_rent:** the median rental price for housing which can provide a measure of central tendency that may be more representative of typical rental prices than the average rent

**unemp_rate:** the percentage of the labor force for the residents who are unemployed and actively seeking employment and this is a key economic indicator for the city

**avg_income: t**he average income earned by residents, which provides insight about the economic prosperity and standard of living within the city

**cost_of_living:** the overall cost of living index, which takes into account factors such as housing, transportation, groceries, healthcare

**price_parity:** the purchasing power parity (PPP) of the city, which provides insight about the relative affordability and purchasing power of residents

**commute_time:** the average time that takes for residents to commute to work or other destinations within the city

**median_aqi:** the median value of the Air Quality Index (AQI), which measures the level of air pollution and its potential impact on health

**transit_score:** the score that evaluates the accessibility and quality of public transportation

The fourth and final data set included in this project is sourced from Kaggle (<https://www.kaggle.com/datasets/polartech/number-of-houses-on-sale-in-all-cities-in-the-us>). This data set gives information about only house sale prices.

**avg_price:** the average sale price of houses within the city, which gives insight about the real estate landscape in that area

## ANALYSIS

```{r, warning = FALSE, message = FALSE}
library(rvest)     
library(tidyverse)
library(janitor)
```

```{r}
html1 <- read_html("https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population") 
web1 <- html_table(html1)
target <- web1[[3]] |>
  data.frame() |>
  slice(-1) |>
  clean_names() |>
  mutate(city = str_remove(city, "\\[.\\]")) |>
  rename_with(~str_remove(., "^x"), starts_with("x")) |>
  select(-starts_with("2022"),
         -change,
         -location) |>
  rename(land_area_mi2 = '2020_land_area',
         land_area_km2 = '2020_land_area_1',
         population = '2020census',
         density_mi2 = '2020_density',
         density_km2 = '2020_density_1',
         state_code = st) |>
  mutate(across(everything(), ~ str_replace_all(., ",", ""))) |>
  mutate_at(c("population", "land_area_mi2", "land_area_km2", "density_mi2", "density_km2"), as.numeric) |>
  select(-land_area_mi2,
         -density_mi2,
         -density_km2)

head(target)
```

```{r}
csv1 <- read.csv("/Users/ilaydabekircan/Documents/SPRING'23/STAT5125/Project/us-cities-demographics.csv", sep = ";")

features2 <- csv1 |>
  clean_names() |>
  pivot_wider(names_from = race,
              values_from = count) |>
  clean_names() |>
  rename(american = american_indian_and_alaska_native,
         hispanic = hispanic_or_latino,
         black = black_or_african_american,
         male = male_population,
         female = female_population) |>
  mutate(male = round(male/(male+female),4),
         female = 1-male) |>
  mutate(white = white/(white + american + hispanic + black + asian),
         american = american/(white + american + hispanic + black + asian),
         hispanic = hispanic/(white+american+hispanic+black+asian),
         black = black/(white + american + hispanic + black + asian),
         asian = asian/(white + american + hispanic + black + asian)) |>
  mutate(perc_veterans = number_of_veterans / total_population,
         perc_foreign_born = foreign_born / total_population) |>
  select(-total_population,
         -number_of_veterans,
         -foreign_born)

features2
```

```{r}
csv2 <- read.csv("/Users/ilaydabekircan/Documents/SPRING'23/STAT5125/Project/us_cities.csv")
features3 <- csv2 |>
  clean_names() |>
  select(-x,
         -latitude,
         -longitude) |>
  mutate(region = as.factor(region),
         size = as.factor(size),
         avg_rent = as.double(avg_rent)) |>
  select(-population,
         -bike_score,
         -walk_score) # more than 75% of null values for bike_score and walk_score

features3
```

```{r}
# https://www.kaggle.com/datasets/polartech/number-of-houses-on-sale-in-all-cities-in-the-us

csv3 <- read.csv("/Users/ilaydabekircan/Documents/SPRING'23/STAT5125/Project/estate of city.csv")

features4 <- csv3 |>
  clean_names() |>
  select(-count)

features4
```

```{r}
trial1 <- target |>
  left_join(features2,
             by = c("city", "state_code")) |>
  left_join(features4,
            by = c("city"))

trial1

trial2 <- target |>
  left_join(features2,
             by = c("city", "state_code")) |>
  left_join(features3,
             by = c("city", "state")) |>  
  left_join(features4,
            by = c("city"))

trial2
```

```{r}
library(naniar)
vis_miss(trial1)
vis_miss(trial2)
```

```{r}
#imputation based on KNN
library(VIM)
trial1 <- select(trial1, -c(city, state_code, state))
trial1 |> miss_var_summary()

knn_impute_t1 <- trial1 |>
 nabular(only_miss = TRUE) |>
 kNN(variable = c("land_area_km2" ,"median_age", "male", "female", "average_household_size", "white", "american", "hispanic", "black", "asian", "perc_veterans", "perc_foreign_born", "avg_price"),
     dist_var = c("land_area_km2" ,"median_age", "male", "female", "average_household_size", "white", "american", "hispanic", "black", "asian", "perc_veterans", "perc_foreign_born", "avg_price"))

knn_impute_t1 <- knn_impute_t1 |> 
  select(-ends_with("NA"),
         -ends_with("imp"))

knn_impute_t1 |> miss_var_summary()
knn_impute_t1
```

```{r}
#imputation based on KNN
trial2 <- select(trial2, -c(city, state_code, state))
trial2 |> miss_var_summary()

knn_impute_t2 <- trial2 |>
 nabular(only_miss = TRUE) |>
 kNN(variable = c("land_area_km2", "median_age", "male", "female", "average_household_size", "white", "american", "hispanic", "black", "asian", "perc_veterans", "perc_foreign_born", "region", "size", "avg_rent", "median_rent", "unemp_rate", "avg_income", "cost_of_living", "price_parity", "commute_time", "median_aqi", "transit_score", "avg_price"),
     dist_var = c("land_area_km2", "median_age", "male", "female", "average_household_size", "white", "american", "hispanic", "black", "asian", "perc_veterans", "perc_foreign_born", "region", "size", "avg_rent", "median_rent", "unemp_rate", "avg_income", "cost_of_living", "price_parity", "commute_time", "median_aqi", "transit_score", "avg_price"))

knn_impute_t2 <- knn_impute_t2 |> 
  select(-ends_with("NA"),
         -ends_with("imp"))

knn_impute_t2 |> miss_var_summary()
knn_impute_t2
```

```{r}
#EDA
library(ggplot2)

summary(knn_impute_t1$population)
summary(knn_impute_t2$population)

# For dataset1
ggplot(knn_impute_t1, aes(x = population)) +
  geom_histogram(fill = "skyblue", color = "black", bins = 30) +
  labs(title = "Population Distribution", x = "Population", y = "Frequency")+
  coord_cartesian(xlim = c(0, max(knn_impute_t1$population)))

ggplot(knn_impute_t1, aes(x = median_age)) +
  geom_histogram(fill = "lightgreen", color = "black", bins = 30) +
  labs(title = "Median Age Distribution", x = "Median Age", y = "Frequency")+theme_bw()

# For dataset2
ggplot(knn_impute_t2, aes(x = population)) +
  geom_histogram(fill = "skyblue", color = "black", bins = 30) +
  labs(title = "Population Distribution", x = "Population", y = "Frequency")+theme_bw()

ggplot(knn_impute_t2, aes(x = median_age)) +
  geom_histogram(fill = "lightgreen", color = "black", bins = 30) +
  labs(title = "Median Age Distribution", x = "Median Age", y = "Frequency")+theme_bw()

```

```{r}
#Correlation
# For dataset1
library(corrplot)
correlation_matrix1 <- cor(select(knn_impute_t1, where(is.numeric)))
corrplot(correlation_matrix1, method = "circle")

# For dataset2
correlation_matrix2 <- cor(select(knn_impute_t2, where(is.numeric)))
corrplot(correlation_matrix2, method = "circle")


```

```{r}
# For dataset1
ggplot(knn_impute_t1, aes(x = population, y = avg_price)) +
  geom_point() +
  labs(title = "Population vs. Median Age", x = "Population", y = "Median Age")+theme_bw()

# For dataset2
ggplot(knn_impute_t2, aes(x = population, y = avg_price)) +
  geom_point() +
  labs(title = "Population vs. Median Age", x = "Population", y = "Median Age")+theme_bw()
```

```{r}
# For dataset1
pairs(~population + median_age + male + female, data = knn_impute_t1)

# For dataset2
pairs(~population + median_age + male + female, data = knn_impute_t2)

```

```{r}
library(tidymodels)
library(tidyverse)

set.seed(3) 
pop_split <- initial_split(knn_impute_t1,
                           prop = 0.9, 
                           strata = population)
pop_train <- pop_split |>
 training()
pop_test <- pop_split |>
 testing()
```

```{r}
# dataset 2
set.seed(3)
pop_split2 <- initial_split(knn_impute_t2, 
                            prop = 0.9, 
                            strata = population)
pop_train_2 <- pop_split2 |>
 training()
pop_test_2 <- pop_split2 |>
 testing()
```

```{r}
# model 1 (linear regression)

population_parsnip_1 <- linear_reg() |> 
  set_mode("regression") |>
  set_engine("lm")

population_workflow_1 <- workflow() |>
  add_model(population_parsnip_1) |>
  add_formula(population ~ .)
 
# model 2 (linear regression with PCA)
population_recipe_2 <- recipe(population ~ .,
                              data = pop_train) |>
  step_normalize(all_numeric_predictors()) |>
  step_pca(all_numeric_predictors(), num_comp = 3) |>
  step_dummy(all_nominal_predictors()) 

population_recipe_2_2 <- recipe(population ~ .,
                                data = pop_train_2) |>
  step_normalize(all_numeric_predictors()) |>
  step_pca(all_numeric_predictors(), num_comp = 3) |>
  step_dummy(all_nominal_predictors()) 

population_workflow_2 <- workflow() |>
  add_model(population_parsnip_1) |>
  add_recipe(population_recipe_2)

population_workflow_2_2 <- workflow() |>
  add_model(population_parsnip_1) |>
  add_recipe(population_recipe_2_2)

# model 3 (linear regression with penalty 0.5)
population_recipe_3 <- recipe(population ~ .,
                       data = pop_train) |>
  step_normalize(all_numeric_predictors()) |>
  step_pca(all_numeric_predictors(), num_comp = 30) |>
  step_dummy(all_nominal_predictors()) 

population_recipe_3_2 <- recipe(population ~ .,
                         data = pop_train_2) |>
  step_normalize(all_numeric_predictors()) |>
  step_pca(all_numeric_predictors(), num_comp = 30) |>
  step_dummy(all_nominal_predictors()) 

population_parsnip_3 <- linear_reg(penalty = 0.5) |> 
  set_mode("regression") |>
  set_engine("glmnet")
 
population_workflow_3 <- workflow() |>
 add_model(population_parsnip_3) |>
 add_recipe(population_recipe_3)

population_workflow_3_2 <- workflow() |>
 add_model(population_parsnip_3) |>
 add_recipe(population_recipe_3_2)

# model 4 (kknn with 5 neighbors)

library(kknn)
population_parsnip_4 <- nearest_neighbor() |> 
  set_mode("regression") |>
  set_engine("kknn", neighbors = 5)

population_workflow_4 <- workflow() |>
 add_model(population_parsnip_4) |>
 add_formula(population ~ .)
 
#model 5
library(ranger)
population_parsnip_5 <- rand_forest() |> 
  set_mode("regression") |>
  set_engine("ranger")

population_workflow_5<- workflow() |>
  add_model(population_parsnip_5) |>
  add_formula(population ~ .)

```

```{r}
workflow_names <- c("lm", 
                    "lm_PCA",
                    "lm_PCA_lasso",
                    "knn",
                    "rf")

workflow_objects <- list(population_workflow_1,
                         population_workflow_2,
                         population_workflow_3,
                         population_workflow_4,
                         population_workflow_5)

workflow_objects_2 <- list(population_workflow_1,
                           population_workflow_2_2,
                           population_workflow_3_2,
                           population_workflow_4,
                           population_workflow_5)


workflows_tbl <- tibble(work_names = workflow_names,
                        work_objects = workflow_objects)
workflows_tbl

workflows_tbl_2 <- tibble(work_names = workflow_names,
                          work_objects = workflow_objects_2)
workflows_tbl_2
 
set.seed(1)
workflows_tbl <- workflows_tbl |>
  rowwise() |>
  mutate(fits = list(fit(work_objects,pop_train))) |>
  mutate(predictions = list(predict(fits, pop_test)))
workflows_tbl
 
set.seed(1)
workflows_tbl_2 <- workflows_tbl_2 |>
  rowwise() |>
  mutate(fits = list(fit(work_objects, pop_train_2))) |>
  mutate(predictions = list(predict(fits, pop_test_2)))
workflows_tbl_2
```

```{r}
#check the performance 
predictions_tbl  <- workflows_tbl |>
 select(work_names, 
        predictions) |>
 unnest(predictions) |>
 cbind(population = pop_test |>
         pull(population))
predictions_tbl

predictions_tbl_2  <- workflows_tbl_2 |>
 select(work_names, 
        predictions) |>
 unnest(predictions) |>
 cbind(population = pop_test |>
         pull(population))
predictions_tbl_2
```

```{r}
predictions_tbl |>
  ggplot(aes(x = population, 
             y = .pred)) +
  geom_point(alpha = 0.2) +
  facet_wrap(~work_names, nrow = 2) +
  geom_abline(slope = 1, linetype = "dotted", color = "red") +
  xlab("Actual") +
  ylab("Prediction") +
  coord_obs_pred() + 
  theme_bw()

predictions_tbl_2 |>
  ggplot(aes(x = population, 
             y = .pred)) +
  geom_point(alpha = 0.2) +
  facet_wrap(~work_names, nrow = 2) +
  geom_abline(slope = 1, linetype = "dotted", color = "red") +
  xlab("Actual") +
  ylab("Prediction") +
  coord_obs_pred() + 
  theme_bw()

```

```{r}
#Check the performance
population_metrics <- metric_set(yardstick::rmse, 
                                 rsq, 
                                 yardstick::mae)

options(scipen = 50)
predictions_metrics <- predictions_tbl |>
  group_by(work_names) |>
  population_metrics(truth = population, 
                     estimate = .pred) |>
  mutate(.estimate = (round(.estimate, 2))) |>
  mutate(data_set = 1)

predictions_metrics_2 <- predictions_tbl_2 |>
  group_by(work_names) |>
  population_metrics(truth = population, 
                     estimate = .pred) |>
  mutate(.estimate = (round(.estimate, 2))) |>
  mutate(data_set = 2)

predictions <- predictions_metrics |>
  full_join(predictions_metrics_2,
             by = c("work_names", ".metric", ".estimator", "data_set", ".estimate")) |>
  mutate(data_set = as.factor(data_set))

predictions_metrics
predictions_metrics_2
predictions
```

```{r}
# YOUR CODE HERE
predictions |>
  ggplot(aes(y = work_names,
             fill = data_set,
             x = .estimate)) +
  geom_col(position = "dodge") +
  facet_wrap(~.metric,
             scales = "free_x") + 
  theme_bw() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        legend.position = "top") +
  ylab("Model") +
  xlab("Performance Estimate")
```

```{r}
#PCA
#standardize the data

scaled_data1 <- scale(knn_impute_t1)
pca_result1 <- prcomp(scaled_data1, center = TRUE, scale. = TRUE)

str(knn_impute_t1)
str(knn_impute_t2)

numeric_data2 <- select(knn_impute_t2, where(is.numeric))

scaled_data2 <- scale(numeric_data2)
pca_result2 <- prcomp(scaled_data2, center = TRUE, scale. = TRUE)

# Extract principal components
pcs1 <- pca_result1$x
pcs2 <- pca_result2$x
# Extract standard deviations
std_dev1 <- pca_result1$sdev
std_dev2 <- pca_result2$sdev

# Extract proportion of variance explained by each PC
variance_explained1 <- pca_result1$sdev^2 / sum(pca_result1$sdev^2)

variance_data1 <- data.frame(
  PC = 1:length(variance_explained1),
  Variance_Explained = variance_explained1
)

variance_explained2 <- pca_result2$sdev^2 / sum(pca_result2$sdev^2)

variance_data2 <- data.frame(
  PC = 1:length(variance_explained2),
  Variance_Explained = variance_explained2
)

variance_data1
variance_data2

combined_table <- bind_rows(variance_data1, variance_data2, .id = "Table")

ggplot(combined_table, aes(x = factor(PC), y = Variance_Explained, fill = factor(Table))) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Variance Explained Plot",
       x = "Principal Component (PC)",
       y = "Variance Explained",
       fill = "Dataset") +
  theme_bw() 

```

## CONCLUSION

We can mention our results, difference between the performance of the models, pca analysis, and which add-on's we have completed.
