---
title: "STAT5125 Project"
format: html
editor: visual
editor_options: 
  chunk_output_type: inline
---

```{r, warning = FALSE, message = FALSE}
library(rvest)     
library(tidyverse)
library(janitor)
```

```{r}
html1 <- read_html("https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population") 
web1 <- html_table(html1)
target <- web1[[3]] |>
  data.frame() |>
  slice(-1) |>
  clean_names() |>
  mutate(city = str_remove(city, "\\[.\\]")) |>
  rename_with(~str_remove(., "^x"), starts_with("x")) |>
  select(-starts_with("2022"),
         -change,
         -location) |>
  rename(land_area_mi2 = '2020_land_area',
         land_area_km2 = '2020_land_area_1',
         population = '2020census',
         density_mi2 = '2020_density',
         density_km2 = '2020_density_1',
         state_code = st) |>
  mutate(across(everything(), ~ str_replace_all(., ",", ""))) |>
  mutate_at(c("population", "land_area_mi2", "land_area_km2", "density_mi2", "density_km2"), as.numeric) |>
  select(-land_area_mi2,
         -density_mi2,
         -density_km2)

target

```

```{r}
#html2 <- read_html("https://en.wikipedia.org/wiki/List_of_United_States_cities_by_area") 
#web2 <- html_table(html2)
#features1 <- web2[[2]] |>
#  data.frame() |>
#  slice(-1) |>
#  select(-starts_with("population")) |>
#  clean_names() |>
#  mutate(city = str_remove(str_remove(city, "\\[.*?\\]"), "\\*+")) |>
#  rename_with(~str_replace(., "1", "km2"), ends_with("_1")) |>
#  rename_with(~str_c(., "_mi2"), ends_with("area")) |>
#  mutate(across(everything(), ~ str_replace_all(., ",", "")))

#features1
```

```{r}
# website link
# https://public.opendatasoft.com/explore/dataset/us-cities-demographics/table/?flg=en-us

csv1 <- read.csv("C:/Users/Hei Yee Lau/Desktop/Uconn/STAT_5125/Project/us-cities-demographics.csv", sep = ";")

features2 <- csv1 |>
  clean_names() |>
  pivot_wider(names_from = race,
              values_from = count) |>
  clean_names() |>
  select(-total_population) |>
  rename(american = american_indian_and_alaska_native,
         hispanic = hispanic_or_latino,
         black = black_or_african_american,
         male = male_population,
         female = female_population) |>
  mutate(male = round(male/(male+female),4),
         female = 1-male) |>
  mutate(white = white/(white+american+hispanic+black+asian),
         american = american/(white+american+hispanic+black+asian),
         hispanic = hispanic/(white+american+hispanic+black+asian),
         black = black/(white+american+hispanic+black+asian),
         asian = asian/(white+american+hispanic+black+asian)) 
  
features2

# for this data set, we should calculate percentage for each race then remove the columns with counts
```

```{r}
# https://www.kaggle.com/datasets/denissad/us-cities
# unemployment rate info is from 2023

csv2 <- read.csv("C:/Users/Hei Yee Lau/Desktop/Uconn/STAT_5125/Project/us_cities.csv")
features3 <- csv2 |>
  clean_names() |>
  select(-x,
         -latitude,
         -longitude) |>
  mutate(region = as.factor(region),
         size = as.factor(size),
         avg_rent = as.double(avg_rent)) |>
  select(-population,
         -bike_score,
         -walk_score,
         -commute_time,
         -median_aqi) #so many null values

features3
```

```{r}
# https://www.kaggle.com/datasets/polartech/number-of-houses-on-sale-in-all-cities-in-the-us
# we don't have year info for this one

csv3 <- read.csv("C:/Users/Hei Yee Lau/Desktop/Uconn/STAT_5125/Project/estate of city.csv")

features4 <- csv3 |>
  clean_names() |>
  select(-count)
  

features4
```

We need to join the dfs by both city and state information because some different states have the same city name. The last data set 'features4' doesn't have state names but the city names are distinct. I checked it below.

```{r}
#features1 |>
#  summarize(rows = n(),
#            distinct_city = n_distinct(city))

features2 |>
  summarize(rows = n(),
            distinct_city = n_distinct(city))

features3 |>
  summarize(rows = n(),
            distinct_city = n_distinct(city))

features4 |>
  summarize(rows = n(),
            distinct_city = n_distinct(city))
```

```{r}
trial1 <- target |>
  left_join(features2,
             by = c("city", "state_code")) |>
  left_join(features4,
            by = c("city"))

trial1

trial2 <- target |>
  left_join(features2,
             by = c("city", "state_code")) |>
  left_join(features3,
             by = c("city", "state")) |>  
  left_join(features4,
            by = c("city"))

trial2
```

```{r}
library(naniar)
vis_miss(trial1)
vis_miss(trial2)
```

```{r}
#imputation based on KNN
library(VIM)
data_drop<-select(trial1, -c(city, state_code, state))
data_drop|>miss_var_summary()


knn_impute <- data_drop |>
 nabular(only_miss = TRUE) |>
 kNN(variable = c("median_age"),
 dist_var = c("population", "land_area_km2"))

knn_impute1 <- knn_impute |>
 kNN(variable = c("male", "female"),
 dist_var = c("population", "land_area_km2", "median_age"))

knn_impute2 <- knn_impute1  |>
 kNN(variable = c("number_of_veterans"),
 dist_var = c("population", "land_area_km2", "median_age","male", "female" ))


knn_impute3 <- knn_impute2 |>
 kNN(variable = c("foreign_born","average_household_size"),
 dist_var = c("population", "land_area_km2", "median_age","male", "female" ,"number_of_veterans"))

knn_impute4 <- knn_impute3 |>
 kNN(variable = c("black","white", "american", "hispanic", "asian", "avg_price"),
 dist_var = c("population", "land_area_km2", "median_age","male", "female" ,"number_of_veterans"))

knn_impute4<-knn_impute4|>select(-c(median_age_NA, male_NA, female_NA, number_of_veterans_NA, foreign_born_NA, average_household_size_NA, black_NA,asian_NA, american_NA, white_NA, hispanic_NA, avg_price_NA, median_age_imp, male_imp, female_imp, number_of_veterans_imp, foreign_born_imp, average_household_size_imp, black_imp, white_imp, american_imp, hispanic_imp, asian_imp, average_household_size_imp, avg_price_imp ))

```


```{r}
#fit the model
lm_model <- lm(population ~ land_area_km2+median_age+male+female+number_of_veterans+foreign_born+average_household_size+black+white+asian+hispanic+american+avg_price, data = knn_impute4)
summary(lm_model)
```

```{r}
library(tidymodels)
library(tidyverse)
#Prediction using workflow and recipe
#split the data 90/10
set.seed(3)
population_split <- initial_split(knn_impute4, 
prop = 0.9, 
strata = population)
population_train <- population_split |>
 training()
population_test <- population_split |>
 testing()
```

```{r}
#model 1
population_parsnip_1 <- linear_reg() |> 
set_mode("regression") |>
 set_engine("lm")
population_workflow_1 <- workflow() |>
 add_model(population_parsnip_1) |>
 add_formula(population ~ .)
 
#model 2
population_recipe_2 <- recipe(population ~ .,
 data = population_train)
population_recipe_2 <- population_recipe_2  |> 
step_normalize(all_numeric_predictors()) |>
 step_pca(all_numeric_predictors(), 
num_comp = 3) |>
 step_dummy(all_nominal_predictors()) 

population_workflow_2 <- workflow() |>
 add_model(population_parsnip_1) |>
 add_recipe(population_recipe_2)


#model 3
population_recipe_3 <- recipe(population ~ .,
 data = population_train)
population_recipe_3 <- population_recipe_3  |> 
step_normalize(all_numeric_predictors()) |>
 step_pca(all_numeric_predictors(), 
num_comp = 30) |>
 step_dummy(all_nominal_predictors())

population_parsnip_3 <- linear_reg(penalty = 0.5) |> 
set_mode("regression") |>
 set_engine("glmnet")
 
population_workflow_3 <- workflow() |>
 add_model(population_parsnip_3) |>
 add_recipe(population_recipe_3)

```

```{r}
#model 4
library(kknn)
population_parsnip_4 <- nearest_neighbor() |> 
set_mode("regression") |>
 set_engine("kknn", neighbors = 5)
population_workflow_4 <- workflow() |>
 add_model(population_parsnip_4) |>
 add_formula(population ~ .)
 
#model 5
library(ranger)
population_parsnip_5 <- rand_forest() |> 
  set_mode("regression") |>
  set_engine("ranger")
population_workflow_5<- workflow() |>
  add_model(population_parsnip_5) |>
  add_formula(population ~ .)

```

```{r}
workflow_names <- c("lm", 
                 "lm_PCA",
                 "lm_PCA_lasso",
                 "knn",
                 "rf")


workflow_objects <- list(population_workflow_1,
                           population_workflow_2,
                           population_workflow_3,
                           population_workflow_4,
                           population_workflow_5)


workflows_tbl <- tibble(work_names = workflow_names,
                        work_objects = workflow_objects)
 workflows_tbl
 
 
 
set.seed(1)
 workflows_tbl <- workflows_tbl |>
  rowwise() |>
  mutate(fits = list(fit(work_objects, 
                         population_train)))
 workflows_tbl
 
 
 
 workflows_tbl <- workflows_tbl |>
 mutate(predictions = list(predict(fits,
 population_test)))
 workflows_tbl

```


```{r}
#check the performance 
predictions_tbl  <- workflows_tbl |>
 select(work_names, 
predictions) |>
 unnest(cols = c(predictions))


predictions_tbl <- predictions_tbl |>
 cbind(population = population_test |>
 pull(population))

 predictions_tbl |>
 ggplot(aes(x = population, 
y = .pred)) +
 geom_point(alpha = 0.2) +
 facet_wrap(~work_names, nrow = 2) +
 geom_abline(slope = 1, linetype = "dotted", color = "red") +
 coord_obs_pred()+theme_bw()

```

```{r}
#Check the performance
population_metrics <- metric_set(yardstick::rmse, rsq, yardstick::mae)

 predictions_tbl |>
 filter(work_names == "lm") |>
 population_metrics(truth = population, 
estimate = .pred)

 
 
predictions_metrics <- predictions_tbl |>
 group_by(work_names) |>
 population_metrics(truth = population, estimate = .pred)


predictions_metrics  |> ggplot(aes(y = work_names, 
x = .estimate, 
fill = work_names)) + 
geom_col() +
 facet_wrap(~.metric, 
scales = "free_x")+theme_bw()

```
